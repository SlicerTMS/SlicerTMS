<!DOCTYPE html>
<html lang='en'>
<head>
    <title>3D Slicer Web Server Experiments</title>
    <meta name='viewport' content='width=device-width, initial-scale=1, user-scalable=no'>
    <meta name='mobile-web-app-capable' content='yes'>
    <meta name='apple-mobile-web-app-capable' content='yes'>
    <!-- <script src="./three.min.js"></script> -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.15.0/dist/tf.min.js"></script>
	<!-- Require the peer dependencies of face-landmarks-detection. -->
	<script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh"></script>
	<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>

	<!-- You must explicitly require a TF.js backend if you're not using the TF.js union bundle. -->
	<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh"></script>
	<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection"></script>
    <link href='./stylesheets/application.css' rel='stylesheet'>
    <script src="https://unpkg.com/vtk.js"></script>
</head>
<body>
<div id="overlay">
    <header>
        <details open>
            <summary>Slicer WebXR Overlay</summary>
            <p>WebXR TMS <a class="back" href="./index.html">Back</a>
            </p>
            <div id="session-info"></div>
            <div id="pose"></div>
            <div id="warning-zone"></div>
            <button id="xr-button" class="barebones-button" disabled>XR not found</button>
        </details>
    </header>
</div>
<video id="video" style="display:none;"></video>
<main style='text-align: center;'>
    <p>Click 'Enter AR' to see content</p>
</main>
<script type="importmap">
        {
            "imports": {
                "three": "./build/three.module.js",
                "three/addons/": "./jsm/"
            }
        }
    </script>
	<script type="module">
    import * as THREE from 'three';
    import { VTKLoader } from 'three/addons/loaders/VTKLoader.js';
    import { STLLoader } from 'three/addons/loaders/STLLoader.js';
    import { sRGBEncoding } from 'three'; // Import sRGBEncoding constant
    // console.log(faceLandmarksDetection)
    import { FaceMeshFaceGeometry } from "./jsm/face.js";


    // Create WebSocket connection
    let socket = null;
    // console.log(document.location.protocol);
    if (document.location.protocol === 'https:') {
        socket = new WebSocket("wss://127.0.0.1:2016/websocket");
    }
    socket.onerror = function (error) {
        alert(`[error] ${error.message}`);
    };

    // XR globals.
    let xrButton = document.getElementById('xr-button');
    let xrSession = null;
    let xrRefSpace = null;
    let currentPose = null;
    let currentURL = "Nothing sent";

    // WebGL scene globals.
    let gl = null;
    let camera, scene, renderer, controller;
    let faceTrackingMesh;
    let width = 0;
    let height = 0;

    const faceGeometry = new FaceMeshFaceGeometry();
    console.log("faceGeometry loaded!");

    function checkSupportedState() {
        navigator.xr.isSessionSupported('immersive-ar').then((supported) => {
            if (supported) {
                xrButton.innerHTML = 'Enter AR';
            } else {
                xrButton.innerHTML = 'AR not found';
            }
            xrButton.disabled = !supported;
        });
    }

	  function initXR() {
	    if (!window.isSecureContext) {
	        document.getElementById("warning-zone").innerText = "WebXR unavailable due to insecure context";
	    }

	    if (navigator.xr) {
	        xrButton.addEventListener('click', onButtonClicked);
	        navigator.xr.addEventListener('devicechange', checkSupportedState);
	        checkSupportedState();
	    }
	}

    function onButtonClicked() {
        if (!xrSession) {
            // Ask for an optional DOM Overlay, see https://immersive-web.github.io/dom-overlays/
            navigator.xr.requestSession('immersive-ar', {
                optionalFeatures: ['dom-overlay'],
                domOverlay: {root: document.getElementById('overlay')}
            }).then(onSessionStarted, onRequestSessionError);
        } else {
            xrSession.end();
        }
    }


        function loadFaceTrackingMesh() {
          return new Promise((resolve, reject) => {
            const loader = new STLLoader();
            loader.load(
              "./gm.stl",
              (geometry) => {
                geometry.center();
                geometry.computeVertexNormals();

                const material = new THREE.MeshLambertMaterial({ color: 0xffc0cb });
                const mesh = new THREE.Mesh(geometry, material);

                // Set the initial scale and rotation
                mesh.scale.set(0.002, 0.002, 0.002);
                mesh.rotation.x = -Math.PI / 2;
                mesh.position.set(0, 0, -1);

                // Set the value of faceTrackingMesh and add it to the scene
                faceTrackingMesh = mesh;
                scene.add(faceTrackingMesh);

                resolve(mesh);
              },
              undefined,
              (error) => {
                console.error("An error occurred while loading the model", error);
                reject(error);
              }
            );
          });
        }




      async function initializeAR() {
        xrButton.innerHTML = 'Exit AR';
        if (xrSession.domOverlayState) {
            document.getElementById('session-info').innerHTML = 'DOM Overlay type: ' + xrSession.domOverlayState.type;
        }

        xrSession.addEventListener('end', onSessionEnded);
        let canvas = document.createElement('canvas');
        gl = canvas.getContext('webgl', { xrCompatible: true });
        xrSession.updateRenderState({ baseLayer: new XRWebGLLayer(xrSession, gl) });

        await xrSession.requestReferenceSpace('local').then((refSpace) => {
            xrRefSpace = refSpace;
            xrSession.requestAnimationFrame(onXRFrame);
        });
        scene = new THREE.Scene();

        renderer = new THREE.WebGLRenderer({antialias: true, alpha: true, // Make the background of the canvas transparent
            preserveDrawingBuffer: true,
            canvas: canvas, context: gl
        });

        renderer.xr.enabled = true;
        renderer.xr.setReferenceSpaceType('local');
        renderer.setSize(window.innerWidth, window.innerHeight);
        renderer.setPixelRatio(window.devicePixelRatio);
        renderer.outputEncoding = THREE.sRGBEncoding; // Set the output encoding to sRGB

        // Add an ambient light to the scene
        const ambientLight = new THREE.AmbientLight(0xffffff, 0.5);
        scene.add(ambientLight);

        // Add a directional light to the scene
        const directionalLight = new THREE.DirectionalLight(0xffffff, 0.5);
        directionalLight.position.set(1, 1, 1);
        scene.add(directionalLight);

        // camera = new THREE.PerspectiveCamera(60, window.innerWidth / window.innerHeight, 0.05, 10);
        camera = new THREE.PerspectiveCamera(70, window.innerWidth / window.innerHeight, 0.01, 100);

        // camera = new THREE.OrthographicCamera(1, 1, 1, 1, -1000, 1000);
        camera.updateProjectionMatrix();
        camera.matrixAutoUpdate = true;

        controller = renderer.xr.getController( 0 );
        scene.add( controller );
        window.addEventListener("resize", () => {resize();});

        // Load the 3D object and add it to the scene
        try {
            await loadFaceTrackingMesh();
        } catch (error) {
            console.error('Error loading 3D object:', error);
        }

        initFaceTracking(renderer, camera);
    }

      function onSessionStarted(session) {
          xrSession = session;
          initializeAR();
      }




    async function initFaceTracking(renderer, camera) {
        const video = document.getElementById('video');
        const stream = await navigator.mediaDevices.getUserMedia({video: true});
        video.srcObject = stream;
        await video.play();

        if (width !== video.videoWidth || height !== video.videoHeight) {
          const w = video.videoWidth;
          const h = video.videoHeight;
          camera.left = -0.5 * w;
          camera.right = 0.5 * w;
          camera.top = 0.5 * h;
          camera.bottom = -0.5 * h;
          camera.updateProjectionMatrix();
          width = w;
          height = h;
          resize();
          faceGeometry.setSize(w, h);
        }

        // const w = video.videoWidth;
        // const h = video.videoHeight;
        console.log('Video set up!')
        // faceGeometry.setSize(w, h);

        // const model = faceLandmarksDetection.SupportedModels.MediaPipeFaceMesh;
        // const detectorConfig = {
        //     runtime: 'mediapipe', // or 'tfjs'
        //     solutionPath: 'https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh',
        // };
        // const detector = await faceLandmarksDetection.createDetector(model, detectorConfig);
        // load facemesh model
        const model = await facemesh.load({ maxFaces: 1 });

        async function detectFaces() {
        const predictions = await model.estimateFaces(video);

              if (predictions.length > 0) {
                faceGeometry.update(predictions[0], false);
                console.log(faceGeometry)

                const track = faceGeometry.track(151, 107, 336);

                // Calculate the new position in world coordinates
                const updatedPosition = new THREE.Vector3(track.position.x, track.position.y, track.position.z);
                updatedPosition.multiplyScalar(0.001); // Scale down the position values
                updatedPosition.z = -updatedPosition.z; // Flip the Z-axis

                // Update the faceTrackingMesh position
                faceTrackingMesh.position.copy(updatedPosition);

                console.log('this is position:', track.position);
                console.log('facemesh:', faceTrackingMesh.position);
                // Increase the scaling factor
                // Update the scale of the 3D object based on the calculated scale factors
                // faceTrackingMesh.scale.set(0.002, 0.002, 0.002);

                faceTrackingMesh.rotation.setFromRotationMatrix(track.rotation);
              }

        requestAnimationFrame(detectFaces);
      }

          detectFaces();
      }

    function resize() {
        const videoAspectRatio = width / height;
        const windowWidth = window.innerWidth;
        const windowHeight = window.innerHeight;
        const windowAspectRatio = windowWidth / windowHeight;
        let adjustedWidth;
        let adjustedHeight;
        if (videoAspectRatio > windowAspectRatio) {
          adjustedWidth = windowWidth;
          adjustedHeight = windowWidth / videoAspectRatio;
        } else {
          adjustedWidth = windowHeight * videoAspectRatio;
          adjustedHeight = windowHeight;
        }
        renderer.setSize(adjustedWidth, adjustedHeight);
        camera.aspect = videoAspectRatio;
        camera.updateProjectionMatrix();
    }


    function onRequestSessionError(ex) {
        alert("Failed to start immersive AR session.");
        console.error(ex.message);
    }

    function onEndSession(session) {
        session.end();
    }

    function onSessionEnded(event) {
        xrSession = null;
        xrButton.innerHTML = 'Enter AR';
        document.getElementById('session-info').innerHTML = '';
        gl = null;
    }

    function onXRFrame(t, frame) {
        let session = frame.session;
        session.requestAnimationFrame(onXRFrame);
        gl.bindFramebuffer(gl.FRAMEBUFFER, session.renderState.baseLayer.framebuffer);
        // retrieve pose of device:
        let pose = frame.getViewerPose(xrRefSpace);
        if (pose) {
            const p = pose.transform.position;
            const m = pose.transform.matrix;
            document.getElementById('pose').innerText = "" +
                "Position: " + 1000 * p.x.toFixed(3) + ", " + 1000 * p.y.toFixed(3) + ", " + 1000 * p.z.toFixed(3) + "\n" +
                "matrix:\n" +
                m[0].toFixed(3) + ", " + m[4].toFixed(3) + ", " + m[8].toFixed(3) + ", " + m[12].toFixed(3) + "\n" +
                m[1].toFixed(3) + ", " + m[5].toFixed(3) + ", " + m[9].toFixed(3) + ", " + m[13].toFixed(3) + "\n" +
                m[2].toFixed(3) + ", " + m[6].toFixed(3) + ", " + m[10].toFixed(3) + ", " + m[14].toFixed(3) + "\n" +
                m[3].toFixed(3) + ", " + m[7].toFixed(3) + ", " + m[11].toFixed(3) + ", " + m[15].toFixed(3) + "\n" + currentURL;
                // console.log(m)
            if (currentPose == null) {
                currentPose = pose;
                sendTracker();
            }
            currentPose = pose;

        } else {
            document.getElementById('pose').innerText = "Position: (null pose)";
        }
        renderer.render(scene, camera);
    }

    function sendTracker() {
        if (socket === null) {
            return
        }
        let pose = currentPose;
        if (pose) {
            let putURL ="";
            const p = pose.transform.position;
            const o = pose.transform.orientation;
            const m = pose.transform.matrix;
            // synchronizes the phone position correctly with the coil:
            let q = new THREE.Quaternion(o.x, o.y, o.z, o.w);
            let newO = q.multiply(new THREE.Quaternion().setFromAxisAngle(new THREE.Vector3(1, 0, 0), Math.PI / 2));
            putURL += `?p=${1000 * p.x},${1000 * p.y},${1000 * p.z}`;
            putURL += `&q=${newO.w},${newO.x},${newO.y},${newO.z}`;
            let ma = `${m}`;
            socket.send(putURL);
            currentURL = putURL;
        }
    }

    setInterval(sendTracker, 200);

    initXR();
</script>
</body>
</html>
